{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 189.7065\n",
      "Epoch 2/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 39.0340\n",
      "Epoch 3/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.6680\n",
      "Epoch 4/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9401\n",
      "Epoch 5/5000\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8184\n",
      "Epoch 6/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5943\n",
      "Epoch 7/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5451\n",
      "Epoch 8/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5301\n",
      "Epoch 9/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5216\n",
      "Epoch 10/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5145\n",
      "Epoch 11/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.5077\n",
      "Epoch 12/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5009\n",
      "Epoch 13/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4942\n",
      "Epoch 14/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4875\n",
      "Epoch 15/5000\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4808\n",
      "Epoch 16/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4741\n",
      "Epoch 17/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4674\n",
      "Epoch 18/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4608\n",
      "Epoch 19/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4542\n",
      "Epoch 20/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4476\n",
      "Epoch 21/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4410\n",
      "Epoch 22/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4344\n",
      "Epoch 23/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4279\n",
      "Epoch 24/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4214\n",
      "Epoch 25/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-28 19:11:47.449982: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4084\n",
      "Epoch 27/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4019\n",
      "Epoch 28/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3954\n",
      "Epoch 29/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3890\n",
      "Epoch 30/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3826\n",
      "Epoch 31/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3762\n",
      "Epoch 32/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3698\n",
      "Epoch 33/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3635\n",
      "Epoch 34/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3571\n",
      "Epoch 35/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3508\n",
      "Epoch 36/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3445\n",
      "Epoch 37/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3382\n",
      "Epoch 38/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3319\n",
      "Epoch 39/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3257\n",
      "Epoch 40/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3194\n",
      "Epoch 41/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3132\n",
      "Epoch 42/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3070\n",
      "Epoch 43/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3008\n",
      "Epoch 44/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2947\n",
      "Epoch 45/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2885\n",
      "Epoch 46/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2824\n",
      "Epoch 47/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2763\n",
      "Epoch 48/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2702\n",
      "Epoch 49/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2641\n",
      "Epoch 50/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2581\n",
      "Epoch 51/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2520\n",
      "Epoch 52/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2460\n",
      "Epoch 53/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2400\n",
      "Epoch 54/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2340\n",
      "Epoch 55/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2280\n",
      "Epoch 56/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2221\n",
      "Epoch 57/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2161\n",
      "Epoch 58/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2102\n",
      "Epoch 59/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2043\n",
      "Epoch 60/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1984\n",
      "Epoch 61/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1926\n",
      "Epoch 62/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1867\n",
      "Epoch 63/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1809\n",
      "Epoch 64/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1751\n",
      "Epoch 65/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1693\n",
      "Epoch 66/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1635\n",
      "Epoch 67/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1577\n",
      "Epoch 68/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1519\n",
      "Epoch 69/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1462\n",
      "Epoch 70/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1405\n",
      "Epoch 71/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1348\n",
      "Epoch 72/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1291\n",
      "Epoch 73/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1234\n",
      "Epoch 74/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1178\n",
      "Epoch 75/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1121\n",
      "Epoch 76/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1065\n",
      "Epoch 77/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1009\n",
      "Epoch 78/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0953\n",
      "Epoch 79/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0897\n",
      "Epoch 80/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0842\n",
      "Epoch 81/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0786\n",
      "Epoch 82/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0731\n",
      "Epoch 83/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0676\n",
      "Epoch 84/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0621\n",
      "Epoch 85/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0566\n",
      "Epoch 86/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0512\n",
      "Epoch 87/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0457\n",
      "Epoch 88/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0403\n",
      "Epoch 89/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0349\n",
      "Epoch 90/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0295\n",
      "Epoch 91/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0241\n",
      "Epoch 92/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0187\n",
      "Epoch 93/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0133\n",
      "Epoch 94/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0080\n",
      "Epoch 95/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0027\n",
      "Epoch 96/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9974\n",
      "Epoch 97/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9921\n",
      "Epoch 98/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9868\n",
      "Epoch 99/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9815\n",
      "Epoch 100/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9763\n",
      "Epoch 101/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9711\n",
      "Epoch 102/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9658\n",
      "Epoch 103/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9606\n",
      "Epoch 104/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9554\n",
      "Epoch 105/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9503\n",
      "Epoch 106/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9451\n",
      "Epoch 107/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9400\n",
      "Epoch 108/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9348\n",
      "Epoch 109/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9297\n",
      "Epoch 110/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9246\n",
      "Epoch 111/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9195\n",
      "Epoch 112/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9145\n",
      "Epoch 113/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9094\n",
      "Epoch 114/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9044\n",
      "Epoch 115/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8993\n",
      "Epoch 116/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8943\n",
      "Epoch 117/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8893\n",
      "Epoch 118/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8843\n",
      "Epoch 119/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8794\n",
      "Epoch 120/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8744\n",
      "Epoch 121/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8695\n",
      "Epoch 122/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8645\n",
      "Epoch 123/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8596\n",
      "Epoch 124/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8547\n",
      "Epoch 125/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8498\n",
      "Epoch 126/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8450\n",
      "Epoch 127/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8401\n",
      "Epoch 128/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8353\n",
      "Epoch 129/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8304\n",
      "Epoch 130/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8256\n",
      "Epoch 131/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8208\n",
      "Epoch 132/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8160\n",
      "Epoch 133/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8113\n",
      "Epoch 134/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8065\n",
      "Epoch 135/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.8017\n",
      "Epoch 136/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7970\n",
      "Epoch 137/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7923\n",
      "Epoch 138/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7876\n",
      "Epoch 139/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7829\n",
      "Epoch 140/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7782\n",
      "Epoch 141/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7735\n",
      "Epoch 142/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7689\n",
      "Epoch 143/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7642\n",
      "Epoch 144/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7596\n",
      "Epoch 145/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7550\n",
      "Epoch 146/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7504\n",
      "Epoch 147/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7458\n",
      "Epoch 148/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7412\n",
      "Epoch 149/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7367\n",
      "Epoch 150/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7321\n",
      "Epoch 151/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7276\n",
      "Epoch 152/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7231\n",
      "Epoch 153/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7186\n",
      "Epoch 154/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7141\n",
      "Epoch 155/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7096\n",
      "Epoch 156/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7051\n",
      "Epoch 157/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.7007\n",
      "Epoch 158/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6962\n",
      "Epoch 159/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6918\n",
      "Epoch 160/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6874\n",
      "Epoch 161/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6830\n",
      "Epoch 162/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6786\n",
      "Epoch 163/5000\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.6742\n",
      "Epoch 164/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6698\n",
      "Epoch 165/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6655\n",
      "Epoch 166/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6611\n",
      "Epoch 167/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6568\n",
      "Epoch 168/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6525\n",
      "Epoch 169/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6482\n",
      "Epoch 170/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6439\n",
      "Epoch 171/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6396\n",
      "Epoch 172/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6353\n",
      "Epoch 173/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6310\n",
      "Epoch 174/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6268\n",
      "Epoch 175/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6226\n",
      "Epoch 176/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6183\n",
      "Epoch 177/5000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6141\n",
      "Epoch 178/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6099\n",
      "Epoch 179/5000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.6058\n",
      "Epoch 180/5000\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "xs = np.array([3,4,5,6,7], dtype=float)\n",
    "ys = np.array([1.11,2.88,5.12,8.03,9.02], dtype=float)\n",
    "model.fit(xs, ys, epochs=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "\n",
      "Args:\n",
      "    x: Input data. It could be:\n",
      "      - A Numpy array (or array-like), or a list of arrays\n",
      "        (in case the model has multiple inputs).\n",
      "      - A TensorFlow tensor, or a list of tensors\n",
      "        (in case the model has multiple inputs).\n",
      "      - A dict mapping input names to the corresponding array/tensors,\n",
      "        if the model has named inputs.\n",
      "      - A `tf.data` dataset. Should return a tuple\n",
      "        of either `(inputs, targets)` or\n",
      "        `(inputs, targets, sample_weights)`.\n",
      "      - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "        or `(inputs, targets, sample_weights)`.\n",
      "      - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a\n",
      "        callable that takes a single argument of type\n",
      "        `tf.distribute.InputContext`, and returns a `tf.data.Dataset`.\n",
      "        `DatasetCreator` should be used when users prefer to specify the\n",
      "        per-replica batching and sharding logic for the `Dataset`.\n",
      "        See `tf.keras.utils.experimental.DatasetCreator` doc for more\n",
      "        information.\n",
      "      A more detailed description of unpacking behavior for iterator types\n",
      "      (Dataset, generator, Sequence) is given below. If using\n",
      "      `tf.distribute.experimental.ParameterServerStrategy`, only\n",
      "      `DatasetCreator` type is supported for `x`.\n",
      "    y: Target data. Like the input data `x`,\n",
      "      it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "      It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "      tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "      or `keras.utils.Sequence` instance, `y` should\n",
      "      not be specified (since targets will be obtained from `x`).\n",
      "    batch_size: Integer or `None`.\n",
      "        Number of samples per gradient update.\n",
      "        If unspecified, `batch_size` will default to 32.\n",
      "        Do not specify the `batch_size` if your data is in the\n",
      "        form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "        (since they generate batches).\n",
      "    epochs: Integer. Number of epochs to train the model.\n",
      "        An epoch is an iteration over the entire `x` and `y`\n",
      "        data provided\n",
      "        (unless the `steps_per_epoch` flag is set to\n",
      "        something other than None).\n",
      "        Note that in conjunction with `initial_epoch`,\n",
      "        `epochs` is to be understood as \"final epoch\".\n",
      "        The model is not trained for a number of iterations\n",
      "        given by `epochs`, but merely until the epoch\n",
      "        of index `epochs` is reached.\n",
      "    verbose: 'auto', 0, 1, or 2. Verbosity mode.\n",
      "        0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        'auto' defaults to 1 for most cases, but 2 when used with\n",
      "        `ParameterServerStrategy`. Note that the progress bar is not\n",
      "        particularly useful when logged to a file, so verbose=2 is\n",
      "        recommended when not running interactively (eg, in a production\n",
      "        environment).\n",
      "    callbacks: List of `keras.callbacks.Callback` instances.\n",
      "        List of callbacks to apply during training.\n",
      "        See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
      "        and `tf.keras.callbacks.History` callbacks are created automatically\n",
      "        and need not be passed into `model.fit`.\n",
      "        `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "        `verbose` argument to `model.fit`.\n",
      "        Callbacks with batch-level calls are currently unsupported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`, and users are\n",
      "        advised to implement epoch-level calls instead with an appropriate\n",
      "        `steps_per_epoch` value.\n",
      "    validation_split: Float between 0 and 1.\n",
      "        Fraction of the training data to be used as validation data.\n",
      "        The model will set apart this fraction of the training data,\n",
      "        will not train on it, and will evaluate\n",
      "        the loss and any model metrics\n",
      "        on this data at the end of each epoch.\n",
      "        The validation data is selected from the last samples\n",
      "        in the `x` and `y` data provided, before shuffling. This argument is\n",
      "        not supported when `x` is a dataset, generator or\n",
      "       `keras.utils.Sequence` instance.\n",
      "        `validation_split` is not yet supported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "    validation_data: Data on which to evaluate\n",
      "        the loss and any model metrics at the end of each epoch.\n",
      "        The model will not be trained on this data. Thus, note the fact\n",
      "        that the validation loss of data provided using `validation_split`\n",
      "        or `validation_data` is not affected by regularization layers like\n",
      "        noise and dropout.\n",
      "        `validation_data` will override `validation_split`.\n",
      "        `validation_data` could be:\n",
      "          - A tuple `(x_val, y_val)` of Numpy arrays or tensors.\n",
      "          - A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n",
      "          - A `tf.data.Dataset`.\n",
      "          - A Python generator or `keras.utils.Sequence` returning\n",
      "          `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n",
      "        `validation_data` is not yet supported with\n",
      "        `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "    shuffle: Boolean (whether to shuffle the training data\n",
      "        before each epoch) or str (for 'batch'). This argument is ignored\n",
      "        when `x` is a generator or an object of tf.data.Dataset.\n",
      "        'batch' is a special option for dealing\n",
      "        with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "        chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "    class_weight: Optional dictionary mapping class indices (integers)\n",
      "        to a weight (float) value, used for weighting the loss function\n",
      "        (during training only).\n",
      "        This can be useful to tell the model to\n",
      "        \"pay more attention\" to samples from\n",
      "        an under-represented class.\n",
      "    sample_weight: Optional Numpy array of weights for\n",
      "        the training samples, used for weighting the loss function\n",
      "        (during training only). You can either pass a flat (1D)\n",
      "        Numpy array with the same length as the input samples\n",
      "        (1:1 mapping between weights and samples),\n",
      "        or in the case of temporal data,\n",
      "        you can pass a 2D array with shape\n",
      "        `(samples, sequence_length)`,\n",
      "        to apply a different weight to every timestep of every sample. This\n",
      "        argument is not supported when `x` is a dataset, generator, or\n",
      "       `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "        as the third element of `x`.\n",
      "    initial_epoch: Integer.\n",
      "        Epoch at which to start training\n",
      "        (useful for resuming a previous training run).\n",
      "    steps_per_epoch: Integer or `None`.\n",
      "        Total number of steps (batches of samples)\n",
      "        before declaring one epoch finished and starting the\n",
      "        next epoch. When training with input tensors such as\n",
      "        TensorFlow data tensors, the default `None` is equal to\n",
      "        the number of samples in your dataset divided by\n",
      "        the batch size, or 1 if that cannot be determined. If x is a\n",
      "        `tf.data` dataset, and 'steps_per_epoch'\n",
      "        is None, the epoch will run until the input dataset is exhausted.\n",
      "        When passing an infinitely repeating dataset, you must specify the\n",
      "        `steps_per_epoch` argument. If `steps_per_epoch=-1` the training\n",
      "        will run indefinitely with an infinitely repeating dataset.\n",
      "        This argument is not supported with array inputs.\n",
      "        When using `tf.distribute.experimental.ParameterServerStrategy`:\n",
      "          * `steps_per_epoch=None` is not supported.\n",
      "    validation_steps: Only relevant if `validation_data` is provided and\n",
      "        is a `tf.data` dataset. Total number of steps (batches of\n",
      "        samples) to draw before stopping when performing validation\n",
      "        at the end of every epoch. If 'validation_steps' is None, validation\n",
      "        will run until the `validation_data` dataset is exhausted. In the\n",
      "        case of an infinitely repeated dataset, it will run into an\n",
      "        infinite loop. If 'validation_steps' is specified and only part of\n",
      "        the dataset will be consumed, the evaluation will start from the\n",
      "        beginning of the dataset at each epoch. This ensures that the same\n",
      "        validation samples are used every time.\n",
      "    validation_batch_size: Integer or `None`.\n",
      "        Number of samples per validation batch.\n",
      "        If unspecified, will default to `batch_size`.\n",
      "        Do not specify the `validation_batch_size` if your data is in the\n",
      "        form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "        (since they generate batches).\n",
      "    validation_freq: Only relevant if validation data is provided. Integer\n",
      "        or `collections.abc.Container` instance (e.g. list, tuple, etc.).\n",
      "        If an integer, specifies how many training epochs to run before a\n",
      "        new validation run is performed, e.g. `validation_freq=2` runs\n",
      "        validation every 2 epochs. If a Container, specifies the epochs on\n",
      "        which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "        validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "    max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "        input only. Maximum size for the generator queue.\n",
      "        If unspecified, `max_queue_size` will default to 10.\n",
      "    workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "        only. Maximum number of processes to spin up\n",
      "        when using process-based threading. If unspecified, `workers`\n",
      "        will default to 1.\n",
      "    use_multiprocessing: Boolean. Used for generator or\n",
      "        `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "        threading. If unspecified, `use_multiprocessing` will default to\n",
      "        `False`. Note that because this implementation relies on\n",
      "        multiprocessing, you should not pass non-picklable arguments to\n",
      "        the generator as they can't be passed easily to children processes.\n",
      "\n",
      "Unpacking behavior for iterator-like inputs:\n",
      "    A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "  tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "  yield not only features (x) but optionally targets (y) and sample weights.\n",
      "  Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "  iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "  second and third elements will be used for y and sample_weight\n",
      "  respectively. Any other type provided will be wrapped in a length one\n",
      "  tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "  should still adhere to the top-level tuple structure.\n",
      "  e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "  features, targets, and weights from the keys of a single dict.\n",
      "    A notable unsupported data type is the namedtuple. The reason is that\n",
      "  it behaves like both an ordered datatype (tuple) and a mapping\n",
      "  datatype (dict). So given a namedtuple of the form:\n",
      "      `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "  it is ambiguous whether to reverse the order of the elements when\n",
      "  interpreting the value. Even worse is a tuple of the form:\n",
      "      `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "  where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "  and sample_weight or passed through as a single element to `x`. As a\n",
      "  result the data processing code will simply raise a ValueError if it\n",
      "  encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "\n",
      "Returns:\n",
      "    A `History` object. Its `History.history` attribute is\n",
      "    a record of training loss values and metrics values\n",
      "    at successive epochs, as well as validation loss values\n",
      "    and validation metrics values (if applicable).\n",
      "\n",
      "Raises:\n",
      "    RuntimeError: 1. If the model was never compiled or,\n",
      "    2. If `model.fit` is  wrapped in `tf.function`.\n",
      "\n",
      "    ValueError: In case of mismatch between the provided input data\n",
      "        and what the model expects or when the input data is empty.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Documents/tflow_env/lib/python3.9/site-packages/keras/engine/training.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0462112]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[3.0000014]], dtype=float32)>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.99999124], dtype=float32)>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac94d0babd387896eb0e257461ac5037be2bc14c6c8d324960fd75f4f0d1494b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
